# parallel-csv
Processing csv files collecting results to map with linked lists

Признаю, что 
1) По части обработки ошибок открытия файла и парсинга значений я допустил многие вольности. 
2) Я не сделал лимитер на количество одновременных открытий файлов в
операционной системе (потому что её ресурсы ограничены).
3) Функция createResultCsv() содержит много дублирующего кода, который хотелось бы сжать.
4) Связный список для синхронизирующей структуры мне показалась не очень хорошей идеей, но зато самой быстро реализуемой, 
хотел воспользоваться чем-то вроде b-tree-plus, но времени не хватило.
5) Нет пока уверенности, что расход памяти не увеличивается с ростом количества и объемов файлов.
6) Как я заметил по результатам бенчмарков, я совершенно неправильно реализовал расспараллеливание. Таким образом, что оно не дает никакого выйгрыша по времени исполнения программы. Это из-за того, что задача распараллелена в лоб.

Всё это не получилось по причине нехватки времени, но я еще допиливаю. Но в целом подход к решению задачи хотел представить именно такой. То есть просто считываем кусочки данных разными горутинами и складываем их в один канал, на другом конце которого эти кусочки складываем нужным нам образом, с учетом количества строк на один ID, количества строк в целом и сортировка по Price. Но парралельность увы, ничего не дает, я снова пошел неправильным способом. А времени допилить правильным способом не хватило.
